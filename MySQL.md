# MySQL

**一条查询语句怎么执行的？**

大体来说，MySQL可以分为Server层和存储引擎层两部分

Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了默认存储引擎。

![](./images/MySQL执行流程.png)

连接器：

```text
连接器负责跟客户端建立连接、获取权限、维持和管理连接。

通过命令或者客户端输入用户名和密码连接MySQL服务器，在完成经典的TCP握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。
- 如果用户名或密码不对，你就会收到一个"Access denied for user"的错误，然后客户端程序结束执行。
- 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。
这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。

客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数wait_timeout控制的，默认值是8小时。
```

查询缓存：

```java
MySQL拿到一个查询请求后，会先到查询缓存看看，之前执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端。如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。

但是大多数情况下建议不要使用查询缓存，因为查询缓存往往弊大于利。
MySQL也提供了这种“按需使用”的方式。你可以将参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样：
mysql> select SQL_CACHE * from T where ID=10；

需要注意的是，MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有这个功能了。
```

分析器：

```java
如果没有命中查询缓存，就要开始真正执行语句了。

分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。

做完“词法分析”后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。
```

优化器：

```java
经过了分析器，在开始执行之前，还要先经过优化器的处理。
优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。
```

执行器：

```text
进入执行器阶段，开始执行语句。
开始执行的时候，要先判断一下你对这个表有没有执行查询的权限，如果没有，就会返回没有权限的错误。
如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。

比如我们举个例子，表中ID字段没有索引且查询使用ID，那么执行器的执行流程是这样的：
1.调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中；
2.调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
3.执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。
至此，这个语句就执行完成了。
对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。
```

**一条更新语句怎么执行的？**

你执行语句前要先连接数据库，这是连接器的工作。

在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表T上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。

接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用哪个索引。然后，执行器负责具体执行，找到这一行，然后更新。 

与查询流程不一样的是，更新流程还涉及两个重要的日志模块：redo log（重做日志）和 binlog（归档日志）

具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做， 

InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写

有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为**crash-safe**。 

 **redo log是InnoDB引擎特有的日志**，而Server层也有自己的日志，称为binlog（归档日志） 

```text
注：
	因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。
```

这两种日志有以下三点不同：

1. redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。
2. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；
   binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”；
3. redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 

执行器和InnoDB引擎在执行这个简单的update语句时的内部流程：

```text
mysql> update T set c=c+1 where ID=1;

1. 执行器先找引擎取ID=1这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=1这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。

2. 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。

3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。

4. 执行器生成这个操作的binlog，并把binlog写入磁盘。

5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。
```

![](./images/update语句的执行流程.png)

redo log用于保证crash-safe能力。innodb_flush_log_at_trx_commit这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。这个参数建议设置成1，这样可以保证MySQL异常重启之后数据不丢失。

sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数也建议设置成1，这样可以保证MySQL异常重启之后binlog不丢失。 



**事务隔离**

ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性） 

SQL标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）

```text
- 读未提交：一个事务还没提交时，它做的变更就能被别的事务看到。
- 读提交：一个事务提交之后，它做的变更才会被其他事务看到。
- 可重复读：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
- 串行化：顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。
```

![事务的隔离性](./images/事务的隔离性.png)

四种隔离级别通过上图分析结果

```text
- 若隔离级别是“读未提交”， 则V1的值就是2。这时候事务B虽然还没有提交，但是结果已经被A看到了。因此，V2、V3也都是2。“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；
- 若隔离级别是“读提交”，则V1是1，V2的值是2。事务B的更新在提交后才能被A看到。所以， V3的值也是2。在“读提交”隔离级别下，视图是在每个SQL语句开始执行的时候创建的，访问的时候以视图的逻辑结果为准。
- 若隔离级别是“可重复读”，则V1、V2是1，V3是2。之所以V2还是1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。
- 若隔离级别是“串行化”，则在事务B执行“将1改成2”的时候，会被锁住。直到事务A提交后，事务B才可以继续执行。所以从A的角度看， V1、V2值是1，V3的值是2。“串行化”隔离级别下直接用加锁的方式来避免并行访问。
```

```text
注：
	在不同的隔离级别下，数据库行为是有所不同的。Oracle数据库的默认隔离级别其实就是“读提交”，因此对于一些从Oracle迁移到MySQL的应用，为保证数据库隔离级别的一致，你一定要记得将MySQL的隔离级别设置为“读提交”。
```

**事务隔离的实现**

在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 

![回滚日志](./images/回滚日志.png)

```text
如上图，假设一个值从1被按顺序改成了2、3、4，在回滚日志里面就会有类似下面的记录。
当前值是4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。如图中看到的，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于read-view A，要得到1，就必须将当前值依次执行图中所有的回滚操作得到。同时你会发现，即使现在有另外一个事务正在将4改成5，这个事务跟read-view A、B、C对应的事务是不会冲突的。

注：
	回滚日志不会一直保留，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。就是当系统里没有比这个回滚日志更早的read-view的时候，回滚日志就会被删除。

建议：
	尽量不要使用长事务，长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。
	在MySQL 5.5及以前的版本，回滚日志是跟数据字典一起放在ibdata文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。
```

**事务的启动方式**

MySQL的事务启动方式有以下两种：

1. 显式启动事务语句， begin 或 start transaction。配套的提交语句是commit，回滚语句是rollback
2. set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行commit 或 rollback 语句，或者断开连接。 

```text
注：
	建议使用set autocommit=1, 通过显式语句的方式来启动事务。
	在autocommit为1的情况下，用begin显式启动的事务，如果执行commit则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。
```



**索引**

索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。

**索引的常见模型**

1. 哈希表

   ```text
   一种以键-值（key-value）存储数据的结构，我们只要输入待查找的值即key，就可以找到其对应的值即Value。
   好处是增加新的数据时速度会很快，只需要往后追加。但缺点是，因为不是有序的，所以哈希索引做区间查询的速度是很慢的。
   所以，哈希表这种结构适用于只有等值查询的场景，比如Memcached及其他一些NoSQL引擎。而有序数组在等值查询和范围查询场景中的性能就都非常优秀。
   ```

2. 有序数组索引

   ```text
   有序数组就是按照某字段递增的顺序保存的。这时候如果你要查某个字段对应的数据，用二分法就可以快速得到，这个时间复杂度是O(log(N))。
   仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。
   所以，有序数组索引只适用于静态存储引擎，比如你要保存的是2017年某个城市的所有人口信息，这类不会再修改的数据。
   ```

3. 二叉搜索树

   ```text
   二叉树虽然是搜索效率最高的，但是实际上大多数数据库存储却并不使用二叉树，其原因是因为索引并不只存在内存中，而且还要写入磁盘。
   举个例子：
    可以想象一下一棵100万节点的平衡二叉树，树高20。一次查询可能需要访问20个数据块。在机械硬盘时代，从磁盘随机读一个数据块需要10 ms左右的寻址时间。也就是说，对于一个100万行的表，如果使用二叉树来存储，单独访问一个行可能需要20个10 ms的时间，这个查询可真够慢的。
   
   为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N叉”树。这里，“N叉”树中的“N”取决于数据块的大小。
   ```
   



```text
不管是哈希还是有序数组，或者N叉树，它们都是不断迭代、不断优化的产物或者解决方案。数据库技术发展到今天，跳表、LSM树等数据结构也被用于引擎设计中
总之数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据库，我们需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景。
```



**InnoDB 的索引模型**

在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。

 每个索引在innoDB里面都对应的有一颗B+树

```text
create table T(
id int primary key, 
k int not null, 
index (k))engine=InnoDB;

举例：上表主键列key和k上有索引
row1~row5的(ID,k)值分别为(100,1)、(200,2)、(300,3)、(500,5)和(600,6)
对应有以下的两颗B+树
```

![innoDB索引结构](./images/innoDB索引结构.png)

```text
根据叶子节点的内容，索引类型分为主键索引和非主键索引。

主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）。
非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引（secondary index）。

例如：SELECT * FROM t WHERE k=5
首先根据普通索引找到主键的值，在根据主键的值在ID索引树中搜索一次，这个过程称为回表。
即：基于非主键索引树需要多扫描一颗索引树，因此我们在应用中尽量使用主键搜索。
```

**索引维护**

B+树为了保证有序性，在插入或着删除数据的时候会对树进行维护。在此过程中会出现两种情况：页分裂和也合并，这两种情况都会有性能损耗。但是如果是自增主键就不会出现这两种情况。

在自增主键的插入数据模式中，每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。然而有业务的字段作为主键，则往往不容易保证插入的有序性，这样写数据的成本相对较高。除了考虑性能外，在非主键索引树叶子结点存储的是主键的值，如果主键采用的是很长的字符串，将会占用很多的存储空间，而如果使用整数类型作为主键，只占用4个字节，显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间就越小
索引从性能和存储空间考虑，往往自增主键索引是更合理的选择。

但是也可以采用业务字段作为索引，例如以下场景：

```text
1. 只有一个索引
2. 该索引必须是唯一索引

由于没有其他索引，所以也不用考虑其他索引的叶子节点大小问题。
这时候应该优先考虑“尽量使用主键查询”的原则，直接将该字段设为主键，避免了每次查询需要搜索两棵树
```

**覆盖索引**

```text
SELECT * FROM T WHERE k BETWEEN 3 AND 5;
SELECT ID FROM T WHERE K BETWEEN 3 AND 5;
以上两条SQL的执行流程有所不同：
第一条：
	1. 在k索引树上找到3对应的ID值，再到ID索引树中搜索对应的行
	2. 再到k索引树取到下一个值为5，在回到ID索引树中搜索对应的行
	3. 再回到k索引树取下一个值为6，不满足条件，结束循环
在这个过程中，回到主键索引树搜索的过程，我们称为回表。这个查询过程读了k索引树的3条记录，回表了两次。
第二条：
查询结果已经在k索引树上了，因此可以直接提供查询结果，不需要回表，即这个查询，索引k已经"覆盖了"我们查询的需求，称为覆盖索引。

建议：
	由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。
```

**最左前缀原则**

B+树这种索引结构，可以利用索引的“最左前缀”，来定位记录。只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符。那么在建立联合索引的时候，如何安排索引内的字段顺序呢，该评估标准就是**索引的复用能力**， 因为可以支持最左前缀，所以当已经有了(a,b)这个联合索引后，一般就不需要单独在a上建立索引了。因此，**第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。** 那么，如果既有联合查询，又有基于a、b各自的查询，但查询条件里面只有b的语句，是无法使用(a,b)这个联合索引的，这时候你不得不维护另外一个索引，也就是说需要同时维护(a,b)、(b) 这两个索引。这时候，我们要**考虑的原则就是空间**了。比如以下情况，a字段是比b字段大的 ，那我就建议你创建一个（a,b)的联合索引和一个(b)的单字段索引。

**索引下推**

对于那些不符合左前缀

```text
例如：
联合索引(a,b,c)，如果既有条件a也有条件c，那么根据前缀规则则会通过该索引搜索树根据条件a搜索结果，然后在MySQL5.6之前，只能再进行回表查询，到主键索引上找出数据行，在进行对比条件c
而在MySQL5.6及以后就引入了索引下推优化，可以在索引遍历过程中，对索引中	包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表的次数
也就是例子中的判断条件a，同时也判断条件c，对于不满足的直接跳过，进而可以减少回表的次数
```



```text
注：
为什么需要重建索引?
索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。

	对于InnoDB表T，如果你要重建索引k，你的两个SQL语句可以这么写：
        alter table T drop index k;
        alter table T add index(k);
    重建非主键索引是合理的，可以达到省空间的目的。
    
    如果重建主键索引，可以如下方式：
        alter table T drop primary key;
        alter table T add primary key(id);
    重建主键的过程不合理。不论是删除主键还是创建主键，都会将整个表重建。
    这两个语句，你可以用这个语句代替 ： alter table T engine=InnoDB。
```

**全局锁和表锁**

根据加锁的范围，MySQL里面的锁大致可以分成全局锁、表级锁和行锁三类

**全局锁**

全局锁就是对整个数据库实例加锁，MySQL提供了一个加全局读锁的方法，命令是 `Flush tables with read lock (FTWRL)`。使用这个命令之后其他例如数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句等都会使线程阻塞。

全局锁的典型使用场景是做**全库逻辑备份**。
但是让整个库只读，会有以下缺点：
1. 如果是在主库上备份，那么在备份期间不能执行更新类的语句，业务流程基本停止。
2. 如果是在从库上备份，那么在备份期间不能执行主库同步过来的binlog，会导致主从延迟。

MySQL数据库自带的逻辑备份工具是mysqldump。当mysqldump使用参数`–single-transaction`的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。**但是前提是引擎要支持这个隔离级别(可重复读)**。
例如，对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用FTWRL命令了。

```text
注：
    single-transaction方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过FTWRL方法。这往往是DBA要求业务开发人员使用InnoDB替代MyISAM的原因之一。

尽量不使用set global readonly=true的方式使全库只读，原因如下：
1. 在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改global变量的方式影响面更大
2. 在异常处理机制上会有所不同，执行FTWRL命令之后如果客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。
```

**表级锁**

MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。

表锁的语法是 lock tables … read/write。与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。

```text
例如，如果在某个线程A中执行lock tables t1 read, t2 write; 这个语句，则其他线程写t1、读写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操作。连写t1都不允许，也不能访问其他表。

对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。

表锁一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有lock tables这样的语句，比较可能的情况是：
- 要么是系统现在还在用MyISAM这类不支持事务的引擎，那要安排升级换引擎；
- 要么是引擎升级了，但是代码还没升级。最后业务开发就是把lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。
```

MySQL 5.5版本中引入了另一类表级的锁是MDL(metadata Lock)，MDL不需要显式使用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性。
- 读锁之间不互斥，因此可以有多个线程同时对一张表增删改查。
- 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

```text
例如一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上。
当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。

虽然MDL锁是系统默认会加的，但却要注意MDL会直到事务提交才释放，所以在做表的结构变更的时候，要注意不要导致锁住线上查询和更新。
```

**行锁**

行锁就是针对数据表中行记录的锁。MySQL的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如MyISAM引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB是支持行锁

```text
两阶段锁协议

在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。

知道了两阶段锁，使用事务的时候需要注意的是：如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的行锁的申请时机尽量往后放。这样一来最可能影响并发度的行锁在事务中待的时间就是最少，这就最大程度地减少了事务之间的锁等待，提升了并发度。
```

**死锁和死锁检测**

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。

两种策略：

1. 直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置，在InnoDB中，innodb_lock_wait_timeout的默认值是50

2. 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。innodb_deadlock_detect的默认值本身就是on

```text
主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。

每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。如果所有事务都要更新同一行，每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。因此，你就会看到CPU利用率很高，但是每秒却执行不了几个事务。

所以这种热点行更新导致的性能问题的症结在于，死锁检测要耗费大量的CPU资源。

三种解决方案：
1. 如果能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉，带有一定的风险性。
2. 控制并发度。如果你有中间件，可以考虑在中间件实现；如果能修改MySQL源码，基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了；
3. 可以考虑通过将一行改成逻辑上的多行来减少锁冲突。每次要更改该数据行的时候，随机选其中一条记录来更新。这样每次冲突概率变成原来的1/10，可以减少锁等待个数，也就减少了死锁检测的CPU消耗。
```

**"快照"在MVCC里的工作原理**

在MySQL里，有两个“视图”的概念
1. 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view … ，而它的查询方法与表一样。
2. 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。

```text
在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。
```

数据行的多版本

```text
InnoDB里面每个事务有一个唯一的事务ID，叫作transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的。
每行数据都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction id赋值给这个数据版本的事务ID，记为row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够在需要的时候根据当前版本和undo log计算出来得到旧数据。也就是说，数据表中的一行记录，其实可能有多个版本(row)，每个版本有自己的row trx_id。
```

一致性视图

```text
InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。
数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。
这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。
```

而数据版本的可见性规则，就是基于数据的row trx_id和这个一致性视图的对比结果得到的。

![数据版本的可见性规则](./images/数据版本的可见性规则.png)

这个视图数组把所有的row trx_id 分成了几种不同的情况

```text
当前事务的启动瞬间来说，一个数据版本的row trx_id，有以下几种可能：
1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
3. 如果落在黄色部分
    a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；
    b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。
```
InnoDB利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。

![事务A,B,C的执行流程](./images/事务A,B,C的执行流程.png)
```text
注：
    该例子中autocommit=1，事务C没有显式地使用begin/commit，表示这个update语句本身就是一个事务，语句完成的时候会自动提交。
    事务的启动时机: begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。想要马上启动一个事务，可以使用命令start transaction with consistent snapshot。

分析执行流程：由上图可见，事务A的视图数组就是[99,100], 事务B的视图数组是[99,100,101], 事务C的视图数组是[99,100,101,102]。
1. 第一个有效更新是事务C，把数据从(1,1)改成了(1,2)。这时候，这个数据的最新版本的row trx_id是102，而90这个版本已经成为了历史版本。
2. 第二个有效更新是事务B，把数据从(1,2)改成了(1,3)。这时候，这个数据的最新版本（即row trx_id）是101，而102又成为了历史版本。(在事务A查询的时候，其实事务B还没有提交，但是它生成的(1,3)这个版本已经变成当前版本了，但这个版本对事务A必须是不可见的。)
3. 事务A要读数据，它的视图数组是[99,100]。读数据都是从当前版本读起的。所以，事务A查询语句的读数据流程是这样的：
    a. 找到(1,3)的时候，判断出row trx_id=101，比高水位大，处于红色区域，不可见。
    b. 找到上一个历史版本，一看row trx_id=102，比高水位大，处于红色区域，不可见。
    c. 再往前找，终于找到了（1,1)，它的row trx_id=90，比低水位小，处于绿色区域，可见。
这样执行下来，虽然期间这一行数据被修改过，但是事务A不论在什么时候查询，看到这行数据的结果都是一致的，所以我们称之为一致性读。

小结：
一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：
1. 版本未提交，不可见。
2. 版本已提交，但是是在视图创建后提交的，不可见。
3. 版本已提交，而且是在视图创建前提交的，可见。

如果此处事务隔离是读提交：
(1,3)还没提交，属于上面情况1，不可见；
(1,2)提交了，属于上面情况3，可见。
所以，这时候事务A查询语句返回的是k=2。事务B查询结果k=3。
```

**更新逻辑**
上图所示：
如果事务B在更新之前查询一次数据，这个查询返回的k的值确实是1。
但是，当它要去更新数据的时候，就不能再在历史版本上更新了，否则事务C的更新就丢失了。因此，事务B此时的set k=k+1是在（1,2）的基础上进行的操作。
有这样一条规则：**更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。**(除了update语句外，select语句如果加锁(不是很理解)，也是当前读。)

```text
把事务A的查询语句select * from t where id=1修改一下，加上lock in share mode 或 for update，也都可以读到版本号是101的数据，返回的k的值是3。下面这两个select语句，就是分别加了读锁（S锁，共享锁）和写锁（X锁，排他锁）。
mysql> select k from t where id=1 lock in share mode;
mysql> select k from t where id=1 for update;
```
在执行事务B中更新后的查询语句的时候，一看自己的版本号是101，最新数据的版本号也是101，是自己的更新，可以直接使用，所以查询得到的k的值是3。

![事务A,B,C'的执行流程](./images/事务A,B,C'的执行流程.png)
```text
如上图，假设事务C不是马上提交的，而是变成了下面的事务C'。
在这个阶段中，"两阶段锁协议"就起到作用了。事务C'没提交，也就是说(1,2)这个版本上的写锁还没释放。而事务B是当前读，必须要读最新版本，而且必须加锁，因此就被锁住了，必须等到事务C'释放这个锁，才能继续它的当前读。
```

可重复读的核心就是一致性读（consistent read），而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：
- 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
- 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。
```text
说明：
    “start transaction with consistent snapshot; ”的意思是从这个语句开始，创建一个持续整个事务的一致性快照。所以，在读提交隔离级别下，这个用法就没意义了，等效于普通的start transaction。
```

```text 
小结：
InnoDB的行数据有多个版本，每个数据版本有自己的row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据row trx_id和一致性视图确定数据版本的可见性。

- 对于可重复读，查询只承认在事务启动前就已经提交完成的数据；
- 对于读提交，查询只承认在语句启动前就已经提交完成的数据；

而当前读，总是读取已经提交完成的最新版本。
```

**普通索引和唯一索引**

查询过程
```text
执行查询的语句： select id from T where k=5。
在索引树上查找的过程，先是通过B+树从树根开始，按层搜索到叶子节点。
- 对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一个不满足k=5条件的记录。
- 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。

innoDB的数据是按数据页为单位来读写的。由于引擎是按页读写的，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在InnoDB中，每个数据页的大小默认是16KB。

- 当找到k=5的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。此时两者性能差异不大。
- 如果k=5这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。由此可见此时的普通索引性能消耗明显低于唯一索引。

但是，对于整型字段，一个数据页可以放近千个key，因此出现这种情况的概率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的CPU来说可以忽略不计。
```

更新过程
```text
当需要更新一个数据页时，如果数据页在内存中就直接更新，如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。
```

**change buffer**
```text
它是一种应用在非唯一普通索引页(non-unique secondary index page)不在缓冲池中，对页进行了写操作，并不会立刻将磁盘页加载到缓冲池，而仅仅记录缓冲变更(buffer changes)，等未来数据被读取时，再将数据合并(merge)恢复到缓冲池中的技术。写缓冲的目的是降低写操作的磁盘IO，提升数据库性能。数据读入内存是需要占用buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率。
它也是可以持久化的数据。即change buffer在内存中有拷贝，也会被写入到磁盘上。

merge：将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。

change buffer用的是buffer pool里的内存，因此不能无限增大。change buffer的大小，可以通过参数innodb_change_buffer_max_size来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。

对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入一条记录，就要先判断现在表中是否已经存在该记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用change buffer了。唯一索引的更新是不能使用change buffer，实际上也只有普通索引可以使用。
```

当插入一条新记录到表中，innoDB的处理流程
- 情况一：这个记录要更新的目标页在内存中
    1. 对于唯一索引来说，判断到没有冲突，插入这个值，语句执行结束；
    2. 对于普通索引来说，找到位置后，插入这个值，语句执行结束。
    ```text
    普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的CPU时间。
    ```
- 情况二：这个记录要更新的目标页不在内存中
    1. 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
    2. 对于普通索引来说，则是将更新记录在change buffer，语句执行就结束了。
    ```text
    将数据从磁盘读入内存涉及随机IO的访问，是数据库里面成本最高的操作之一。change buffer因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。
    ```

change buffer 的使用场景
```text
change buffer的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做merge之前，change buffer记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。

对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在change buffer，但之后由于马上要访问这个数据页，会立即触发merge过程。这样随机访问IO的次数不会减少，反而增加了change buffer的维护代价。所以，对于这种业务模式来说，change buffer反而起到了副作用。
```

索引选择
```text
普通索引和唯一索引这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以尽量选择普通索引。
如果所有的更新后面，都马上伴随着对这个记录的查询，那么应该关闭change buffer。而在其他情况下，change buffer都能提升更新性能。
普通索引和change buffer的配合使用，对于数据量大的表的更新优化尤其明显的。
特别地，在使用机械硬盘时，change buffer这个机制的收效是非常显著的。所以，当有一个类似“历史数据”的库，并且出于成本考虑用的是机械硬盘时，那你应该特别关注这些表里的索引，尽量使用普通索引，然后把change buffer 尽量开大，以确保这个“历史数据”表的数据写入速度。
```

![带change buffer的更新过程](./images/带change buffer的更新过程.png)
分析一下插入语句的执行流程：insert into t(id,k) values(id1,k1),(id2,k2);
假设当前k索引树的状态，查找到位置后，k1所在的数据页在内存(InnoDB buffer pool)中，k2所在的数据页不在内存中。
```text
这条插入语句做了如下的操作
1. Page 1在内存中，直接更新内存
2. Page 2没有在内存中，就在内存的change buffer区域，记录下“我要往Page 2插入一行”这个信息
3. 将上述两个动作记入redo log中
做完上面这些，事务就可以完成了。所以执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘（两次操作合在一起写了一次磁盘），而且还是顺序写的。同时，图中的两个虚线箭头，是后台操作，不影响更新的响应时间。

接着执行 select * from t where k in (k1, k2);
读语句发生在更新语句后不久，内存中的数据都还在，那么此时的这两个读操作就与系统表空间（ibdata1）和 redo log（ib_log_fileX）无关了
1. 读Page 1的时候，直接从内存返回。
2. 要读Page 2的时候，需要把Page 2从磁盘读入内存中，然后应用change buffer里面的操作日志，生成一个正确的版本并返回结果。需要读Page 2的时候，这个数据页才会被读入内存。

简单地对比这两个机制在提升更新性能上的收益的话，redo log 主要节省的是随机写磁盘的IO消耗（转成顺序写），而change buffer主要节省的则是随机读磁盘的IO消耗
```


**优化器对索引的选择**

优化器的逻辑

```text
优化器选择索引的目的是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的CPU资源越少。当然，扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。
```

扫描行数的判断
```text
MySQL在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。
这个统计信息就是索引的“区分度”。一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。
show index from t; //显示表t中的索引信息
```

MySQL 索引的基数
```text
MySQL采样统计的方法得到基数，因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太高了，所以只能选择“采样统计”。
采样统计的时候，InnoDB默认会选择N个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。而数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据行数超过1/M的时候，会自动触发重新做一次索引统计。

在MySQL中，有两种存储索引统计的方式，可以通过设置参数innodb_stats_persistent的值来选择：
1. 设置为on的时候，表示统计信息会持久化存储。这时，默认的N是20，M是10。
2. 设置为off的时候，表示统计信息只存储在内存中。这时，默认的N是8，M是16。

由于是采样统计，这个基数都是很容易不准的。
```

优化器预估扫描行
```text
EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的。分析你的查询语句或是表结构的性能瓶颈。

在有些查询中，由于普通索引每次都需要拿着主键值然后回表去查询整行数据，这也是优化器的考虑情况之一，难免优化器会不走索引，直接主键全表扫描。由于优化器没能准确的判断扫描行数，可能会MySQL选错索引。

既然是统计信息不对，那就修正。analyze table t 命令，可以用来重新统计索引信息。如果发现explain的结果预估的rows值跟实际情况差距比较大，可以采用这个方法来处理。如果只是索引统计不准确，通过analyze命令可以解决很多问题。
注意：优化器判断因素不止扫描行数，还有其他因素。
```

索引选择异常和处理
```text
大多数时候优化器都能找到正确的索引，但偶尔会出现索引选错导致SQL没有达到预期的执行效果
1. 采用force index强行选择一个索引。MySQL会根据词法解析的结果分析出可能可以使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫描多少行。如果force index指定的索引在候选索引列表中，就直接选择这个索引，不再评估其他索引的执行代价。
2. 可以考虑修改语句，引导MySQL使用我们期望的索引。
    select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1;
    会采用索引b，因为只需要找出b最小的一行，因为优化器认为使用索引b可以避免排序（b本身是索引，已经是有序的了，如果选择索引b的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为代价更小。
    select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b,a limit 1;
    通过将order by b 改为order by b,a，要求按照b,a排序，就意味着使用这两个索引都需要排序。因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描行数少的索引a。
    select * from  (select * from t where (a between 1 and 1000)  and (b between 50000 and 100000) order by b limit 100)alias limit 1;
    也可以用limit 100让优化器意识到，使用b索引代价是很高的。其实是根据数据特征诱导了一下优化器，也不具备通用性。
3. 在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。
    发现这个优化器错误选择的索引其实根本没有必要存在，于是就删掉了这个索引，优化器也就重新选择到了正确的索引。
```

MySQL慢查询日志
```text
找到mysql的安装目录，找到my.ini文件夹在[mysqld]处加入以下代码开启慢查询，永久有效。
#开启慢查询
slow_query_log = ON
#log-slow-queries:代表MYSQL慢查询的日志存储目录,此目录文件一定要有写权限；
log-slow-queries="D://MySQL//log//mysql-slow.log"
#最长执行时间 (查询的最长时间，超过了这个时间则记录到日志中).
long_query_time = 0
#没有使用到索引的查询也将被记录在日志中
log-queries-not-using-indexes = ON

最后查log文件即可有详细的SQL执行信息
```

**前缀索引**
优势；占用的空间会更小
使用前缀索引后，可能会导致查询语句读数据的次数变多。
使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。

确定字符串索引前缀
```text
在建立索引时关注的是区分度，区分度越高越好。因为区分度越高，意味着重复的键值越少。因此，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。
可以使用下面这个语句，算出这个列上有多少个不同的值：依次选取不同长度的前缀来看这个值
select count(distinct xxx) as L from T;
使用前缀索引很可能会损失区分度，所以需要预先设定一个可以接受的损失比例，比如5%。然后，在返回的中，找出不小于 L * 95%的值。
```

前缀索引对覆盖索引的影响
```text
使用前缀索引可能会增加扫描行数，这会影响到性能。使用前缀索引就用不上覆盖索引对查询性能的优化了，在选择是否使用前缀索引时需要考虑的一个因素。

例如：
alter table T add index index1(c);
alter table T add index index2(c(2));

select a,c from T where c='xx';
select a,b,c from T where c='xx';
如果使用index1（即c整个字符串的索引结构）的话，可以利用覆盖索引，从index1查到结果后直接就返回了，不需要回到ID索引再去查一次。而如果使用index2（即c(2)索引结构）的话，就不得不回到ID索引再去判断c字段的值。
即使你将index2的定义修改为c(20)的前缀索引，这时候虽然index2已经包含了所有的信息，但InnoDB还是要回到id索引再查一下，因为系统并不确定前缀索引的定义是否截断了完整信息。
```

**倒序存储和使用hash字段**
索引选取的越长，占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的效率也就会越低。

- 倒序存储。
- hash字段。可以在表上再创建一个整数字段，来保存某字段的校验码，同时在这个整数hash字段上创建索引。

它们的区别，主要体现在以下三个方面：
```text
1. 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而hash字段方法需要增加一个字段。当然，倒序存储方式使用4个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个hash字段也差不多抵消了。
2. 在CPU消耗方面，倒序方式每次写和读的时候，都需要额外调用一次reverse函数，而hash字段的方式需要额外调用一次crc32()函数。如果只从这两个函数的计算复杂度来看的话，reverse函数额外消耗的CPU资源会更小些。
3. 从查询效率上看，使用hash字段方式的查询性能相对更稳定一些。因为crc32算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。
```
